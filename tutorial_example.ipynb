{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example - IBM Granite models\n",
    "\n",
    "Because of the limited resources, we won't be able to train or fine-tune full models here. Instead, we will demonstrate advantages of using MoE models by comparing inference speed and memory usage of IBM Granite MoE model with a dense model of similar size.\n",
    "\n",
    "IBM Granite is a suite of large language models (LLMs) developed by IBM, release in decemeber 2024. Dense models are available in sizes 2b and 8b parameters, while MoE models are available in sizes 3b-a800m and 1b-a400m parameters. The MoE models use the same architecture as the dense models, but with the addition of Mixture of Experts layers, which allow the model to scale up in size without a proportional increase in computational cost."
   ],
   "id": "e9f17d90cf4a70fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To speed things up, we will use the quantized GGUF version of the models, which can be run on a single GPU with limited memory. The model with 3b parameters in Q4_K_M format weights only 2GB.",
   "id": "8538b09ba1418f8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will use the `llama-cpp-python` library, which is a python binding for the `llama.cpp` C++ library, one of the most popular and efficient inference engines for LLms.",
   "id": "db4a1a1d8c6a177b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:12:29.304640Z",
     "start_time": "2025-11-13T23:12:29.303290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from llama_cpp import Llama"
   ],
   "id": "538c1f957e67baf8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-13T23:12:31.470043Z",
     "start_time": "2025-11-13T23:12:29.349912Z"
    }
   },
   "source": [
    "llm_dense = Llama.from_pretrained(\n",
    "    \"bartowski/granite-3.1-2b-instruct-GGUF\",\n",
    "    filename=\"granite-3.1-2b-instruct-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=-1, # Comment this line to run on CPU\n",
    "    n_ctx=1024,\n",
    "    local_dir=\"models\",\n",
    "    cache_dir=\"cache\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "llm_sparse = Llama.from_pretrained(\n",
    "    \"bartowski/granite-3.1-3b-a800m-instruct-GGUF\",\n",
    "    filename=\"granite-3.1-3b-a800m-instruct-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=-1,  # Comment this line to run on CPU\n",
    "    n_ctx=1024,\n",
    "    local_dir=\"models\",\n",
    "    cache_dir=\"cache\",\n",
    "    verbose=False\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/Documents/umisi/architektury/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now let's compare the inference speed of both models on the same prompt.",
   "id": "22ec76fcdd1d004c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we will test a complex prompt, to showcase the difference in performance.",
   "id": "30f8b5ea012ceced"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:12:31.521725Z",
     "start_time": "2025-11-13T23:12:31.520231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "complex_chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant that explains complex scientific concepts \"\n",
    "            \"in simple terms.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Explain the theory of relativity in simple terms, \"\n",
    "            \"and provide an example of how it applies to everyday life.\"\n",
    "        )\n",
    "    }\n",
    "]"
   ],
   "id": "e46c5f0523a9fdde",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:12:59.779634Z",
     "start_time": "2025-11-13T23:12:31.569282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "completion_dense = llm_dense.create_chat_completion(messages=complex_chat, max_tokens=512, stream=False)\n",
    "print(\"Time taken (dense): %.2f\" % (time.time() - start))"
   ],
   "id": "1527d5131b327bc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (dense): 28.21\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:13:13.917302Z",
     "start_time": "2025-11-13T23:12:59.869284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "completion_sparse = llm_sparse.create_chat_completion(messages=complex_chat, max_tokens=512, stream=False)\n",
    "print(\"Time taken (sparse): %.2f\" % (time.time() - start))"
   ],
   "id": "c83655dd5b565ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (sparse): 14.05\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we see from the results the MoE model is able to generate the completion significantly faster (~3x faster) than the dense model, despite having more parameters. This demonstrates the efficiency of MoE models in handling large-scale language tasks.",
   "id": "53f5e0831db8dca4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's compare the outputs of both models.",
   "id": "5bb785c61147151d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:13:13.967093Z",
     "start_time": "2025-11-13T23:13:13.965500Z"
    }
   },
   "cell_type": "code",
   "source": "print(completion_dense['choices'][0]['message']['content'])",
   "id": "ab3107896feb63d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theory of relativity, proposed by Albert Einstein, is actually made up of two parts: the Special Theory of Relativity and the General Theory of Relativity. Let's break it down into simpler terms:\n",
      "\n",
      "1. Special Theory of Relativity (1905): This theory has two main ideas. First, the laws of physics are the same for all observers moving at a constant speed (not accelerating) relative to each other. Second, the speed of light in a vacuum is constant, no matter how fast you're moving or where you're moving from. This means that as you approach the speed of light, time slows down for you compared to a stationary observer. It's like if you were on a super-fast spaceship and tried to throw a ball; to someone watching from Earth, the ball would appear to move slower and slower as you approached the speed of light.\n",
      "\n",
      "2. General Theory of Relativity (1915): This theory extends the Special Theory of Relativity to include gravity. It states that massive objects cause a distortion in space-time, which we perceive as gravity. Imagine a rubber sheet stretched out - if you place a heavy ball (like the Earth) on it, the ball will create a dip. Now, if you roll a smaller ball (like a planet) near the heavy ball, it will move towards the heavier ball, not because of a direct force, but because it's following the curve of the distorted rubber sheet.\n",
      "\n",
      "Now, let's see how this applies to everyday life:\n",
      "\n",
      "Consider a GPS satellite orbiting the Earth. These satellites are moving at high speeds and are far from the Earth's surface, where gravity is weaker. Due to the General Theory of Relativity, the satellites' clocks tick slightly slower than clocks on the Earth's surface. This might seem insignificant, but it's crucial for accurate positioning. If we didn't account for this time dilation, GPS would provide positions with errors of several kilometers.\n",
      "\n",
      "In essence, the theory of relativity tells us that time and space aren't absolute, but are relative to the observer's motion and the gravitational field they're in. This has profound implications for our understanding of the universe, from the behavior of subatomic particles to the large-scale structure of the cos\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:13:14.015595Z",
     "start_time": "2025-11-13T23:13:14.014125Z"
    }
   },
   "cell_type": "code",
   "source": "print(completion_sparse['choices'][0]['message']['content'])",
   "id": "d1f7ad9c2750e302",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd be happy to explain the theory of relativity in simple terms and provide an everyday example.\n",
      "\n",
      "1. **Special Theory of Relativity (1905)**: This theory, proposed by Albert Einstein, states that the laws of physics are the same for all non-accelerating observers, and that the speed of light in a vacuum is constant, regardless of the motion of the light source or the observer.\n",
      "\n",
      "In simpler terms, it means that:\n",
      "- **Time and Space are Relative**: They don't move at the same speed for everyone. For example, if you're traveling at high speeds, time might appear to move slower for you compared to someone who's stationary. This is known as time dilation.\n",
      "- **Length Contraction**: Objects in motion appear shorter to a stationary observer. So, if you were traveling at a high speed, you might appear shorter to someone standing still.\n",
      "\n",
      "2. **General Theory of Relativity (1915)**: This theory, also by Einstein, is an extension of the special theory of relativity. It introduces the idea that massive objects cause a distortion in space-time, which we perceive as gravity.\n",
      "\n",
      "In simpler terms, it means:\n",
      "- **Gravity is Not a Force**: It's a result of the curvature of space-time caused by mass and energy. So, a planet orbits the sun not because it's pulled by the sun's gravity, but because it's moving along the curved path created by the sun's mass.\n",
      "\n",
      "**Everyday Example**: Imagine you're on a train moving at a constant speed. If you throw a ball straight up, it will appear to fall back to your hand at the same speed it was thrown, even though you're moving. This is because, according to the theory of relativity, time is moving slower for you on the train compared to someone standing still. So, the ball's speed relative to you (and thus, its time) appears slower.\n",
      "\n",
      "This might seem strange, but it's a real phenomenon that has been tested and confirmed by experiments. It's a reminder that our perception of time and space can be quite different depending on how we're moving or how we're observing.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Commentary\n",
    "At the first glance both outputs seem similar, however the dense model provides a more detailed explanation of the theory of relativity, while the MoE model gives a more concise summary. Depending on the application, one might prefer the more detailed response or the more concise one. MoE models may be more suitable for real-time applications where speed is crucial.\n",
    "\n",
    "Next example will highlight the difference in output quality between the two models even more clearly."
   ],
   "id": "a19fe941bf1ca4ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple prompt",
   "id": "a81c10900e6e9187"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will also evaluate both models on a simpler prompt, so that we will easily see the difference in output quality.",
   "id": "8cbf95020a8dc9d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:13:14.066554Z",
     "start_time": "2025-11-13T23:13:14.065195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simple_chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a professional storyteller who creates engaging and imaginative stories.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a four-sentence story about a giraffe who gets a job as a window washer for a skyscraper.\"\n",
    "    }\n",
    "]"
   ],
   "id": "9b90b65d7c3e1b03",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:13:25.680624Z",
     "start_time": "2025-11-13T23:13:14.109873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "completion_dense = llm_dense.create_chat_completion(messages=simple_chat, max_tokens=512, stream=False)\n",
    "print(\"Time taken (dense): %.2f\" % (time.time() - start))"
   ],
   "id": "8447c8bb79372219",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (dense): 11.57\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:13:32.828168Z",
     "start_time": "2025-11-13T23:13:25.768983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "completion_sparse = llm_sparse.create_chat_completion(messages=simple_chat, max_tokens=512, stream=False)\n",
    "print(\"Time taken (sparse): %.2f\" % (time.time() - start))"
   ],
   "id": "450fbca54983b55c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (sparse): 7.06\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:13:32.882871Z",
     "start_time": "2025-11-13T23:13:32.881174Z"
    }
   },
   "cell_type": "code",
   "source": "print(completion_dense['choices'][0]['message']['content'])",
   "id": "41d408e7dcfd2ce8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of a bustling metropolis, a unique giraffe named Gazelle, renowned for his extraordinary neck, found himself yearning for a change of pace. Unbeknownst to his herd, Gazelle had always been captivated by the city's towering skyscrapers, their glass facades reflecting the sun's dazzling dance. One day, a peculiar job posting caught his eye: \"Giraffe Window Washer Needed.\" Intrigued, Gazelle applied, and to his astonishment, was offered the position. With a sturdy safety harness and a newfound appreciation for the city's heights, Gazelle began his unconventional career, washing windows from the 50th floor, his long neck a marvel to the city below, forever changing the perspective of both the giraffe and the urban jungle.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:13:32.931128Z",
     "start_time": "2025-11-13T23:13:32.929511Z"
    }
   },
   "cell_type": "code",
   "source": "print(completion_sparse['choices'][0]['message']['content'])",
   "id": "f39ae565632835af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of a bustling metropolis, a majestic giraffe named Kofi lived. Kofi was not your ordinary giraffe; he had a peculiar dream - to work as a window washer on the city's towering skyscrapers. One day, an opportunity knocked when a local construction company needed a unique window washer for their ambitious project.\n",
      "\n",
      "With a sturdy branch and an unyielding determination, Kofi embarked on his new journey. He scaled the skyscrapers, his long neck reaching heights unimaginable to most. His daily routine was a sight to behold, as he swung from building to building, leaving a trail of sparkling windows.\n",
      "\n",
      "Despite the challenges, Kofi found joy in his work. He became a beloved figure in the city, a symbol of resilience and a reminder that dreams, no matter how unconventional, can turn into reality. His story echoed through the city, inspiring everyone who saw him, proving that even the tallest of creatures could find their place in the world.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Commentary\n",
    "\n",
    "See? For simple prompts, the performance does not make that big difference anymore. However, here the quality of responses does. The dense model produces a more vivid, creative and engagin story, while the MoE model's output is more generic and less imaginative. This highlights that while MoE models are efficient, they may not always match the quality of dense models, especially for creative tasks."
   ],
   "id": "3c84024851c0192f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, we demonstrated the advantages of MoE models in terms of inference speed and memory efficiency compared to dense models of similar size. While MoE models excel in speed and resource usage, dense models may still hold an edge in output quality for certain tasks. The choice between MoE and dense models should be made based on the specific requirements of the application, balancing speed, resource constraints, and output quality.\n",
    "\n",
    "Feel free to experiment with different prompts and tasks to further explore the capabilities of MoE models!"
   ],
   "id": "3a0d49e47e28fcd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# (optional) Build Your Own MoE Model\n",
    "\n",
    "Now that you've seen how MoE models work in practice, it's time to build and train your own! We have a complete implementation of a Mixture-of-Experts Transformer for text classification in this repository.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "The MoE implementation is organized in modular files:\n",
    "- **`models.py`** - MoE architecture (SimpleMoE, SimpleMoEDecoderLayer, SimpleMoETransformer)\n",
    "- **`data.py`** - Dataset loading and preprocessing\n",
    "- **`training.py`** - Training and evaluation functions\n",
    "- **`train.py`** - Main training script\n",
    "- **`visualize_experts.py`** - Visualize expert activations per token\n"
   ],
   "id": "443d6a4519ac4cd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 1: Visualize Expert Activations\n",
   "id": "2322f244679ab01d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T02:14:37.708709Z",
     "start_time": "2025-11-14T02:14:35.923220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from visualize_experts import visualize_text\n",
    "\n",
    "# Visualize a positive review\n",
    "visualize_text(\n",
    "    text=\"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged.\",\n",
    "    all_layers=True,\n",
    "    output_path=\"positive_review.png\"\n",
    ")\n",
    "\n",
    "# Visualize a negative review\n",
    "visualize_text(\n",
    "    text=\"Terrible movie. Boring plot and bad acting. Complete waste of time.\",\n",
    "    all_layers=True,\n",
    "    output_path=\"negative_review.png\"\n",
    ")\n",
    "\n",
    "# Visualize a mixed review\n",
    "visualize_text(\n",
    "    text=\"The cinematography was beautiful but the story was predictable.\",\n",
    "    all_layers=True,\n",
    "    output_path=\"mixed_review.png\"\n",
    ")"
   ],
   "id": "b2f80ba7c67ef667",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This movie was absolutely fantastic! The acting was superb and the plot kept me engaged.\n",
      "Prediction: POS (probability=0.993)\n",
      "Saved combined heatmap to: positive_review.png\n",
      "Text: Terrible movie. Boring plot and bad acting. Complete waste of time.\n",
      "Prediction: NEG (probability=0.002)\n",
      "Saved combined heatmap to: negative_review.png\n",
      "Text: The cinematography was beautiful but the story was predictable.\n",
      "Prediction: POS (probability=0.841)\n",
      "Saved combined heatmap to: mixed_review.png\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions to analyze:**\n",
    "- Do different sentiments activate different experts?\n",
    "- Are certain experts more \"specialized\" (used more often for specific words)?\n",
    "- How does expert selection differ between Layer 0 and Layer 1?\n",
    "- Which tokens activate the most diverse set of experts?"
   ],
   "id": "28ebe22be2628f5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 2: Experiment with Model Architecture (Intermediate)",
   "id": "9d1a7ab0f5fa5dae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions to answer:**\n",
    "- Which configuration gives the best accuracy?\n",
    "- Which configuration trains the fastest?\n",
    "- Is there a \"sweet spot\" for the number of experts?\n",
    "- How much does adding more layers help?\n"
   ],
   "id": "36a9286224f6a6e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 3: Custom Routing Strategy (Challenge)\n",
    "\n",
    "Implement a different expert routing strategy.\n",
    "\n",
    "**Current implementation:** Top-k=2 soft routing (weighted average of top 2 experts)\n",
    "\n",
    "**Your task:** Modify `models.py` to implement one of these alternatives:\n",
    "\n",
    "**Option A: Top-1 Hard Routing**\n",
    "- Select only the single best expert per token\n",
    "- Faster but less smooth\n",
    "\n",
    "**Option B: Load Balancing**\n",
    "- Add auxiliary loss to encourage equal expert usage\n",
    "- Prevents expert collapse (all tokens using same expert)\n",
    "\n",
    "**Option C: Noisy Top-k**\n",
    "- Add noise to routing probabilities during training\n",
    "- Helps exploration and generalization\n"
   ],
   "id": "a997b1a880ed31d8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
